<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="RoboSense Track 4: Cross-Modal Drone Navigation Challenge">
  <meta name="keywords" content="Cross-Modal, Drone Navigation, Computer Vision, Natural Language Processing">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RoboSense: Track 4</title>

  <link href="https://fonts.googleapis.com/css?family=Nunito" rel="stylesheet">

  <link rel="stylesheet" href="./static2/css/bulma.min.css">
  <link rel="stylesheet" href="./static2/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  
  <script defer src="./static2/js/fontawesome.all.min.js"></script>
  <link rel="icon" href="../logo.png" type="image/x-icon">

  <style>
    .a {
      color: #ffc000;
    }
    .columns {
      display: flex; justify-content: flex-start; padding: 5px; text-align: left;
    }
    .tag {
      background-color: #f5f5f5; border-radius: 3px; padding: 5px; margin-right: 5px; font-size: 14px; color: #333;
    }
  </style>

<style>
  a.external-link:hover {
    background-color: rgb(109,109,109) !important;
    border-color: rgb(109,109,109) !important;
  }
  </style>

  <style>
    #backToTopBtn {
      position: fixed; bottom: 20px; right: 50px; z-index: 999;
      background-color: #ffc000; color: white; border: none;
      border-radius: 50%; padding: 12px 16px; font-size: 20px; cursor: pointer;
      box-shadow: 0px 4px 8px rgba(0,0,0,0.2); transition: opacity 0.3s; display: none;
    }
    
    #backToTopBtn:hover {
      background-color: #76b900;
    }
    
    @media (max-width: 768px) {
      #backToTopBtn {
        font-size: 18px; padding: 10px 14px;
      }
    }

    .img-hover-effect {
      transition: transform 0.3s ease-in-out;
    }

    .img-hover-effect:hover {
      transform: scale(1.05); /* Zoom to 105% */
    }
    </style>

</head>

<body>

  <button onclick="scrollToTop()" id="backToTopBtn" title="Go to top">‚Üë</button>
  <script>
    window.onscroll = function() {
      scrollFunction();
    };
    function scrollFunction() {
      const button = document.getElementById("backToTopBtn");
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
        button.style.display = "block";
      } else {
        button.style.display = "none";
      }
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    }
  </script>
  
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item">
          <span class="icon"><i class="fas fa-home"></i></span>
        </a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">RoboSense 2025</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://robosense2025.github.io/" target="_blank">RoboSense 2025 Main Page</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track1" target="_blank">- Track #1: Driving with Language</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track2" target="_blank">- Track #2: Social Navigation</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track3" target="_blank">- Track #3: Sensor Placement</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track4" target="_blank">- Track #4: Cross-Modal Drone Navigation</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track5" target="_blank">- Track #5: Cross-Platform 3D Object Detection</a>
            <a class="navbar-item" href="https://www.iros25.org/" target="_blank">IROS 2025 Main Page</a>
          </div>
        </div>
      </div>
    </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <i class="fas fa-rocket fa-2x" style="color: #76b900; margin-bottom: 12px;"></i>
              <br>
              The <a style="color: #ffc000;">Robo</a><a style="color: #76b900;">Sense</a> Challenge 2025
            </h1>
            <h2 class="title is-2 publication-title">
              Track <a style="color: #76b900;">#4</a>: Cross-Modal Drone Navigation
            </h2>
            <h3 class="title is-4 publication-title" style="color: #666;">
              Towards Natural Language-Guided Cross-View Image Retrieval
            </h3>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" style="margin-top: 0px;">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width left">
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              üëã Welcome to <strong>Track <a style="color: #76b900;">#4</a>: Cross-Modal Drone Navigation</strong> of the <a href="https://robosense2025.github.io/" target="_blank"><strong>2025 RoboSense Challenge</strong></a>!
            </p>
            <p>
              This track focuses on developing robust models for <strong>natural language-guided cross-view image retrieval</strong>, specifically designed for scenarios where input data is captured from drastically different viewpoints such as aerial (drone or satellite) and ground-level perspectives. 
            </p>
            <p>
              Participants will tackle the challenge of creating systems that can effectively retrieve corresponding images from large-scale cross-view databases based on natural language text descriptions, even under common corruptions such as blurriness, occlusions, or sensory noise.
            </p>
            <p>
              üèÜ <b>Prize Pool:</b> <a style="color: #76b900;"><b>$2,000 USD</b></a> (1st: $1,000, 2nd: $600, 3rd: $400) + Innovation Awards
            </p>
          </div>

          <hr>

          <h2 class="title is-3">üéØ Objective</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              This track evaluates the capability of vision-language models to perform cross-modal drone navigation through natural language-guided image retrieval. The key challenges include:
            </p>
            <ul>
              <li><strong>Cross-View Matching</strong>: Retrieve corresponding images across drastically different viewpoints (aerial drone/satellite vs. ground-level)</li>
              <li><strong>Natural Language Understanding</strong>: Process and interpret natural language descriptions to guide image retrieval</li>
              <li><strong>Robustness</strong>: Maintain performance under real-world corruptions including blurriness, occlusions, and sensor noise</li>
              <li><strong>Multi-Platform Support</strong>: Handle imagery from multiple platforms including drone, satellite, and ground cameras</li>
            </ul>
            <p>
              The ultimate goal is to develop models that can enable intuitive drone navigation through natural language commands while being robust to real-world sensing conditions.
            </p>
          </div>

          <hr>

          <h2 class="title is-3">üóÇÔ∏è Phases & Requirements</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <h4 class="title is-4">Phase <a style="color: #76b900;">#1</a>: Public Evaluation</h4>
            <p>
              <strong>Duration:</strong> June <a style="color: #76b900;"><b>15th</b></a>, 2025 (anytime on earth) - August <a style="color: #76b900;"><b>15th</b></a>, 2025 (anytime on earth)
            </p>
            <p>
              This phase involves evaluation on a publicly available test set with approximately <strong>190 test cases</strong> (24GB GPU version). Participants can:
            </p>
            <ul>
              <li>Download the official dataset and baseline model</li>
              <li>Develop and test their approaches using the public test set</li>
              <li>Submit results with reproducible code</li>
              <li>Iterate and improve their models based on public leaderboard feedback</li>
            </ul>

            <h4 class="title is-4">Phase <a style="color: #76b900;">#2</a>: Final Evaluation</h4>
            <p>
              <strong>Duration:</strong> August <a style="color: #76b900;"><b>15th</b></a>, 2025 (anytime on earth) - September <a style="color: #76b900;"><b>15th</b></a>, 2025 (anytime on earth)
            </p>
            <p>
              This phase involves final evaluation on a <strong>private test set</strong> with the same number of test cases (~190) as Phase I. Key requirements:
            </p>
            <ul>
              <li>Submit final model and code for evaluation on the private test set</li>
              <li>Provide technical report describing the approach and innovations</li>
              <li>Include model weights and complete reproducible implementation</li>
              <li>Final rankings and awards will be determined based on Phase II results</li>
            </ul>
          </div>

          <hr>

          <h2 class="title is-3">üóÑÔ∏è Dataset</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              The track uses the <strong>RoboSense Track 4 Cross-Modal Drone Navigation Dataset</strong>, based on the GeoText-1652 benchmark. The dataset features:
            </p>
            <ul>
              <li><strong>Multi-platform imagery</strong>: drone, satellite, and ground cameras</li>
              <li><strong>Large scale</strong>: 100K+ images across 72 universities</li>
              <li><strong>Rich annotations</strong>: global descriptions, bounding boxes, and spatial relations</li>
              <li><strong>No overlap</strong>: Training (33 universities) and test (39 universities) are completely separate</li>
            </ul>
            
            <div class="table-container">
              <table class="table is-bordered is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Platform</th>
                    <th>Split</th>
                    <th>Images</th>
                    <th>Descriptions</th>
                    <th>Bbox-Texts</th>
                    <th>Classes</th>
                    <th>Universities</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><strong>Drone</strong></td>
                    <td>Train</td>
                    <td>37,854</td>
                    <td>113,562</td>
                    <td>113,367</td>
                    <td>701</td>
                    <td>33</td>
                  </tr>
                  <tr>
                    <td><strong>Drone</strong></td>
                    <td>Test</td>
                    <td>51,355</td>
                    <td>154,065</td>
                    <td>140,179</td>
                    <td>951</td>
                    <td>39</td>
                  </tr>
                  <tr>
                    <td><strong>Satellite</strong></td>
                    <td>Train</td>
                    <td>701</td>
                    <td>2,103</td>
                    <td>1,709</td>
                    <td>701</td>
                    <td>33</td>
                  </tr>
                  <tr>
                    <td><strong>Satellite</strong></td>
                    <td>Test</td>
                    <td>951</td>
                    <td>2,853</td>
                    <td>2,006</td>
                    <td>951</td>
                    <td>39</td>
                  </tr>
                  <tr>
                    <td><strong>Ground</strong></td>
                    <td>Train</td>
                    <td>11,663</td>
                    <td>34,989</td>
                    <td>14,761</td>
                    <td>701</td>
                    <td>33</td>
                  </tr>
                  <tr>
                    <td><strong>Ground</strong></td>
                    <td>Test</td>
                    <td>2,921</td>
                    <td>8,763</td>
                    <td>4,023</td>
                    <td>793</td>
                    <td>39</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <hr>

          <h2 class="title is-3">üõ†Ô∏è Baseline Model</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              The baseline model is built upon the <strong>GeoText-1652</strong> framework, which combines:
            </p>
            <ul>
              <li><strong>Vision-Language Architecture</strong>: Based on X-VLM for multi-modal understanding</li>
              <li><strong>Spatial Relation Matching</strong>: Advanced spatial reasoning for cross-view alignment</li>
              <li><strong>Multi-Platform Support</strong>: Handles drone, satellite, and ground imagery</li>
              <li><strong>Robust Training</strong>: Trained on diverse university campuses with rich annotations</li>
            </ul>
            <p>
              The baseline provides a strong foundation for participants to build upon, incorporating state-of-the-art techniques in cross-modal retrieval and spatial understanding.
            </p>
          </div>

          <hr>

          <h2 class="title is-3">üìä Baseline Results</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              The baseline model achieves the following performance on the Phase I evaluation set (~190 test cases):
            </p>
            <div class="table-container">
              <table class="table is-bordered is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Query Type</th>
                    <th>R@1</th>
                    <th>R@5</th>
                    <th>R@10</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><strong>Text Query</strong></td>
                    <td>29.9</td>
                    <td>46.3</td>
                    <td>54.1</td>
                  </tr>
                  <tr>
                    <td><strong>Image Query</strong></td>
                    <td>50.1</td>
                    <td>81.2</td>
                    <td>90.3</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <p>
              <strong>Note</strong>: These results represent the baseline performance on the 24GB GPU version used for Phase I evaluation. Participants are encouraged to develop novel approaches to surpass these benchmarks.
            </p>
          </div>

          <hr>

          <h2 class="title is-3">üîó Resources</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              We provide the following resources to support the development of models in this track:
            </p>
            <div class="table-container">
              <table class="table is-bordered is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Resource</th>
                    <th>Link</th>
                    <th>Description</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><strong>GitHub Repository</strong></td>
                    <td><a href="https://github.com/robosense2025/track4" target="_blank">github.com/robosense2025/track4</a></td>
                    <td>Baseline code and setup instructions</td>
                  </tr>
                  <tr>
                    <td><strong>Dataset</strong></td>
                    <td><a href="https://huggingface.co/datasets/robosense/datasets/tree/main/track4-cross-modal-drone-navigation" target="_blank">HuggingFace Dataset</a></td>
                    <td>Dataset with training and test splits</td>
                  </tr>
                  <tr>
                    <td><strong>Baseline Model</strong></td>
                    <td><a href="https://huggingface.co/truemanv5666/GeoText1652_model" target="_blank">Pre-Trained Checkpoint</a></td>
                    <td>GeoText-1652 baseline model weights</td>
                  </tr>
                  <tr>
                    <td><strong>Registration</strong></td>
                    <td><a href="https://docs.google.com/forms/d/e/1FAIpQLSdwfvk-NHdQh9-REiBLCjHMcyLT-sPCOCzJU-ux5jbcZLTkBg/viewform?usp=sharing&ouid=111184461807593368933" target="_blank">Google Form</a>&nbsp;(Closed on August 15th)</td>
                    <td>Team registration for the challenge</td>
                  </tr>
                  <tr>
                    <td><strong>Evaluation Server</strong></td>
                    <td><a href="https://www.codabench.org/competitions/9219/" target="_blank">CodaLab Platform</a></td>
                    <td>Online evaluation platform</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>




          <hr>

          <h2 class="title is-3">‚ùì Frequently Asked Questions</h2>
          <div class="content has-text-justified" style="padding-top: 0px">

          <p>
            Here, we provide a list of Frequently Asked Questions (FAQs) below for better clarity. 
            If you have additional questions on the details about this competition, please reach out at <a>robosense2025@gmail.com</a>.
          </p>

          <hr>
            
          <div style="margin-bottom: 25px;">
            <h4 class="title is-5" style="color: #76b900; margin-bottom: 10px;">
              Question 1
            </h4>
            <p style="background-color: #f9f9f9; padding: 15px; border-radius: 8px; border-left: 4px solid #76b900;">
              Answer 1
            </p>
          </div>

          <div style="margin-bottom: 25px;">
            <h4 class="title is-5" style="color: #76b900; margin-bottom: 10px;">
              Question 2
            </h4>
            <p style="background-color: #f9f9f9; padding: 15px; border-radius: 8px; border-left: 4px solid #76b900;">
              Answer 2
            </p>
          </div>

          <div style="margin-bottom: 25px;">
            <h4 class="title is-5" style="color: #76b900; margin-bottom: 10px;">
              Question 3
            </h4>
            <p style="background-color: #f9f9f9; padding: 15px; border-radius: 8px; border-left: 4px solid #76b900;">
              Answer 3
            </p>
          </div>


          

          <!-- <hr>

          <h2 class="title is-3">üìß Contact</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              For questions, technical support, or clarifications about Track 4, please contact:
            </p>
            <ul>
              <li><strong>Email</strong>: <a href="mailto:robosense2025@gmail.com">robosense2025@gmail.com</a></li>
              <li><strong>GitHub Issues</strong>: <a href="https://github.com/robosense2025/track4/issues" target="_blank">Submit technical questions</a></li>
              <li><strong>Challenge Website</strong>: <a href="https://robosense2025.github.io/" target="_blank">https://robosense2025.github.io/</a></li>
            </ul>
          </div> -->

          <hr>

          <h2 class="title is-3">üìñ References</h2>
          <div class="content has-text-justified" style="padding-top: 0px;">
            <div style="
              max-width: 100%;
              overflow-x: auto;
              background-color: #f9f9f9;
              border-radius: 5px;
              padding: 1em;
              font-size: 0.9em;
              box-sizing: border-box;
            ">
              <pre style="
                margin: 0;
                font-family: monospace;
                white-space: pre;
                overflow-x: auto;
              ">
@inproceedings{chu2024towards, 
  title     = {Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatial Relation Matching}, 
  author    = {Chu, Meng and Zheng, Zhedong and Ji, Wei and Wang, Tingyu and Chua, Tat-Seng}, 
  booktitle = {European Conference on Computer Vision}, 
  year      = {2024} 
}

@misc{robosense2025track4,
  title        = {RoboSense Challenge 2025: Track 4 - Cross-Modal Drone Navigation},
  author       = {RoboSense Challenge 2025 Organizers},
  year         = {2025},
  howpublished = {https://robosense2025.github.io/track4}
}
              </pre>
            </div>
          </div>

        </div>
      </div>
    </div><br>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International</a> license.
            </p>
            <p>
              Copyright ¬© <a style="color: #ffc000;">Robo</a><a style="color: #76b900;">Sense</a> 2025 All Rights Reserved.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

</body>

</html>
