<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="">
  <meta name="keywords" content="3D Scene Understanding, 3D Visual Grounding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RoboSense: Track 3</title>

  <link href="https://fonts.googleapis.com/css?family=Nunito" rel="stylesheet">

  <link rel="stylesheet" href="./static2/css/bulma.min.css">
  <link rel="stylesheet" href="./static2/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  
  <script defer src="./static2/js/fontawesome.all.min.js"></script>
  <link rel="icon" href="../logo.png" type="image/x-icon">

  <style>
    .a {
      color: #ffc000;
    }
    .columns {
      display: flex; justify-content: flex-start; padding: 5px; text-align: left;
    }
    .tag {
      background-color: #f5f5f5; border-radius: 3px; padding: 5px; margin-right: 5px; font-size: 14px; color: #333;
    }
  </style>

<style>
  a.external-link:hover {
    background-color: rgb(109,109,109) !important;
    border-color: rgb(109,109,109) !important;
  }
  </style>

  <style>
    #backToTopBtn {
      position: fixed; bottom: 20px; right: 50px; z-index: 999;
      background-color: #ffc000; color: white; border: none;
      border-radius: 50%; padding: 12px 16px; font-size: 20px; cursor: pointer;
      box-shadow: 0px 4px 8px rgba(0,0,0,0.2); transition: opacity 0.3s; display: none;
    }
    
    #backToTopBtn:hover {
      background-color: #76b900;
    }
    
    @media (max-width: 768px) {
      #backToTopBtn {
        font-size: 18px; padding: 10px 14px;
      }
    }

    .img-hover-effect {
      transition: transform 0.3s ease-in-out;
    }

    .img-hover-effect:hover {
      transform: scale(1.05); /* Zoom to 105% */
    }
    </style>

</head>




<body>


  <button onclick="scrollToTop()" id="backToTopBtn" title="Go to top">‚Üë</button>
  <script>
    window.onscroll = function() {
      scrollFunction();
    };
    function scrollFunction() {
      const button = document.getElementById("backToTopBtn");
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
        button.style.display = "block";
      } else {
        button.style.display = "none";
      }
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    }
  </script>
  
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item">
          <span class="icon"><i class="fas fa-home"></i></span>
        </a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">RoboSense 2025</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://robosense2025.github.io/" target="_blank">Main Page</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track1" target="_blank">Track 1: Driving with Language</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track2" target="_blank">Track 2: Social Navigation</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track3" target="_blank">Track 3: Sensor Placement</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track4" target="_blank">Track 4: Cross-Modal Drone Navigation</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track5" target="_blank">Track 5: Cross-Platform 3D Object Detection</a>
          </div>
        </div>
      </div>
    </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <i class="fas fa-camera-retro fa-2x" style="color: #76b900; margin-bottom: 12px;"></i>
              <br>
              The <a style="color: #ffc000;">Robo</a><a style="color: #76b900;">Sense</a> Challenge 2025
            </h1>
            <h2 class="title is-2 publication-title">
              Track <a style="color: #76b900;">#3</a>: Sensor Placement
            </h2>
            <h3 class="title is-4 publication-title" style="color: #666;">
              Towards generalizable perception models across sensor placements
            </h3>
            <p style="margin-top: 16px;">
              <a class="button is-medium" 
              style="background-color: #76b900; color: white; border: none;" 
              href="https://www.codabench.org/competitions/9284/" 
              target="_blank">
             üöÄ Submit your results
            </a>
            </p>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" style="margin-top: 0px;">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width left">
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              üëã Welcome to <strong>Track <a style="color: #76b900;">#3</a>: Cross-Sensor Placement 3D Object Detection</strong> of the <a href="https://robosense2025.github.io/" target="_blank"><strong>2025 RoboSense Challenge</strong></a>!
            </p>
            <p>
              Autonomous driving models typically rely on well-calibrated sensor setups, and even slight deviations in
              sensor placements across different vehicles or platforms can significantly degrade performance.
              This lack of robustness to sensor variability makes it challenging to transfer perception models between
              different vehicle platforms without extensive retraining or manual adjustment.
              Thus, achieving generalization across sensor placements is essential for the practical deployment of
              driving perception models.
            </p>
            <p>
              üèÜ <strong>Prize Pool</strong>: $2,000 USD (1st: $1,000, 2nd: $600, 3rd: $400) + Innovation Awards
            </p>
            <p>
              <img src="./images/track3/track3.png" alt="Track 3 Image" class="img-hover-effect" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
            </p>
          </div>

          <hr>

          <h2 class="title is-3">üéØ Objective</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              This track evaluates the capability of generalizing perception models across sensor placements. Participants
              are expected to develop models that, when trained on fixed sensor placements, can generalize to diverse
              configurations with minimal performance degradation.
            </p>
          </div>

          <hr>

          <h2 class="title is-3">üóÇÔ∏è Phases & Requirements</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <h4 class="title is-4">Phase 1: Development and Validation</h4>
            <p>
              <strong>Duration:</strong> 15 June 2025 ‚Äì 15 August 2025
            </p>
            <p>
              In this phase, participants are provided with the <strong>training</strong> and <strong>validation</strong>
              datasets which include diverse LiDAR placements. Ground truth annotations are made available to support
              method development and performance validation.
              Participants are expected to use these datasets to design, train, and refine their models.
            </p>

            <h4 class="title is-4">Phase 2: Testing and Submission</h4>
            <p>
              <strong>Duration:</strong> 15 August 2025 ‚Äì 15 October 2025
            </p>
            <p>
              In the second phase, participants will have access to a <strong>test</strong> dataset with LiDAR placements
              different from those in the training and validation sets. Ground truth annotations are not provided.
              They are required to run their developed models on the test set and submit the results in the json format.
              Model performance will be evaluated based on the submitted outputs.
            </p>
          </div>

          <hr>

          <h2 class="title is-3">üöó Dataset</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              This track uses datasets collected in the CARLA simulator and formatted according to the <strong>nuScenes</strong> data structure.
              Please download the provided <a href="YOUR_DEVKIT_LINK_HERE">development kit</a> to process the datasets.
              This kit is based on the <code>nuscenes-devkit</code> but has been customized for this track.
              It can be used in the same way as the official nuScenes dataset.
              <strong>Do not</strong> install the official <code>nuscenes-devkit</code>, as it may not be fully compatible with the datasets used in this track.
            </p>
          </div>

          <hr>

          <h2 class="title is-3">üõ†Ô∏è Baseline Model</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              In this track, BEVFusion-L is used as the baseline method. Participants may choose to build upon it or
              develop their own approaches independently. Detailed environment setup and experimental protocols can be found in the
              <a href="https://github.com/robosense2025/track3" target="_blank" rel="noopener noreferrer"><strong>Track3 GitHub repository</strong></a>.
            </p>
          </div>

          <hr>

          <h2 class="title is-3">üìä Baseline Results</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              In this track, we use ...
            </p>
            <div class="table-container">
              <table class="table is-bordered is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Metric</th>
                    <th>Metric 1</th>
                    <th>Metric 2</th>
                    <th>Metric 3</th>
                    <th>Metric 4</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Baseline Model</td>
                    <td>x.xx</td>
                    <td>x.xx</td>
                    <td>x.xx</td>
                    <td>x.xx</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <hr>

          <h2 class="title is-3">üîó Resources</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              We provide the following resources to support the development of models in this track:
            </p>
            <div class="table-container">
              <table class="table is-bordered is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Resource</th>
                    <th>Link</th>
                    <th></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>GitHub</td>
                    <td><a href="https://github.com/robosense2025/track3" target="_blank">https://github.com/robosense2025/track3</a><td>
                  </tr>
                  <tr>
                    <td>Checkpoint<td>
                    <td></td>
                  </tr>
                  <tr>
                    <td><strong>Registration</strong></td>
                    <td><a href="https://docs.google.com/forms/d/e/1FAIpQLSdwfvk-NHdQh9-REiBLCjHMcyLT-sPCOCzJU-ux5jbcZLTkBg/viewform?usp=sharing&ouid=111184461807593368933" target="_blank">Google Form</a></td>
                  </tr>
                  <tr>
                    <td><strong>Evaluation Server</strong></td>
                    <td><a href="https://www.codabench.org/competitions/9284/" target="_blank">Codabench</a></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <hr>





<!-- <h2 class="title is-3">üìñ References</h2>
<div class="content has-text-justified" style="padding-top: 0px;">
  <div style="
    max-width: 100%;
    overflow-x: auto;
    background-color: #f9f9f9;
    border-radius: 5px;
    padding: 1em;
    font-size: 0.9em;
    box-sizing: border-box;
  ">
    <pre style="
      margin: 0;
      font-family: monospace;
      white-space: pre;
      overflow-x: auto;
    ">
@article{xie2025drivebench,
  title     = {},
  author    = {},
  booktitle = {},
  year      = {2025},
  url       = {https://robosense2025.github.io}
}
    </pre>
  </div>
</div> -->




        </div>
      </div>
    </div>
  </section>



  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International</a> license.
            </p>
            <p>
              Copyright ¬© <a style="color: #ffc000;">Robo</a><a style="color: #76b900;">Sense</a> 2025 All Rights Reserved.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>


</body>

</html>
