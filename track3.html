<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="">
  <meta name="keywords" content="3D Scene Understanding, 3D Visual Grounding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RoboSense: Track 3</title>

  <link href="https://fonts.googleapis.com/css?family=Nunito" rel="stylesheet">

  <link rel="stylesheet" href="./static2/css/bulma.min.css">
  <link rel="stylesheet" href="./static2/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  
  <script defer src="./static2/js/fontawesome.all.min.js"></script>
  <link rel="icon" href="../logo.png" type="image/x-icon">

  <style>
    .a {
      color: #ffc000;
    }
    .columns {
      display: flex; justify-content: flex-start; padding: 5px; text-align: left;
    }
    .tag {
      background-color: #f5f5f5; border-radius: 3px; padding: 5px; margin-right: 5px; font-size: 14px; color: #333;
    }
  </style>

<style>
  a.external-link:hover {
    background-color: rgb(109,109,109) !important;
    border-color: rgb(109,109,109) !important;
  }
  </style>

  <style>
    #backToTopBtn {
      position: fixed; bottom: 20px; right: 50px; z-index: 999;
      background-color: #ffc000; color: white; border: none;
      border-radius: 50%; padding: 12px 16px; font-size: 20px; cursor: pointer;
      box-shadow: 0px 4px 8px rgba(0,0,0,0.2); transition: opacity 0.3s; display: none;
    }
    
    #backToTopBtn:hover {
      background-color: #76b900;
    }
    
    @media (max-width: 768px) {
      #backToTopBtn {
        font-size: 18px; padding: 10px 14px;
      }
    }

    .img-hover-effect {
      transition: transform 0.3s ease-in-out;
    }

    .img-hover-effect:hover {
      transform: scale(1.05); /* Zoom to 105% */
    }
    </style>

</head>




<body>


  <button onclick="scrollToTop()" id="backToTopBtn" title="Go to top">‚Üë</button>
  <script>
    window.onscroll = function() {
      scrollFunction();
    };
    function scrollFunction() {
      const button = document.getElementById("backToTopBtn");
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
        button.style.display = "block";
      } else {
        button.style.display = "none";
      }
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    }
  </script>
  
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item">
          <span class="icon"><i class="fas fa-home"></i></span>
        </a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">RoboSense 2025</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://robosense2025.github.io/" target="_blank">RoboSense 2025 Main Page</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track1" target="_blank">- Track #1: Driving with Language</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track2" target="_blank">- Track #2: Social Navigation</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track3" target="_blank">- Track #3: Sensor Placement</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track4" target="_blank">- Track #4: Cross-Modal Drone Navigation</a>
            <a class="navbar-item" href="https://robosense2025.github.io/track5" target="_blank">- Track #5: Cross-Platform 3D Object Detection</a>
            <a class="navbar-item" href="https://www.iros25.org/" target="_blank">IROS 2025 Main Page</a>
          </div>
        </div>
      </div>
    </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <i class="fas fa-camera-retro fa-2x" style="color: #76b900; margin-bottom: 12px;"></i>
              <br>
              The <a style="color: #ffc000;">Robo</a><a style="color: #76b900;">Sense</a> Challenge 2025
            </h1>
            <h2 class="title is-2 publication-title">
              Track <a style="color: #76b900;">#3</a>: Sensor Placement
            </h2>
            <h3 class="title is-4 publication-title" style="color: #666;">
              Towards Generalizable 3D Object Detection Across Sensor Placements
            </h3>
            <p style="margin-top: 16px;">
              <a class="button is-medium" 
              style="background-color: #76b900; color: white; border: none;" 
              href="https://www.codabench.org/competitions/9284/" 
              target="_blank">
             üöÄ Submit on CodaBench
            </a>
            </p>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" style="margin-top: 0px;">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width left">
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              <img src="./images/track3/track3.png" alt="Track 3 Image" style="max-width: 100%; height: auto;">
            </p>
            <p>
              üëã Welcome to <strong>Track <a style="color: #76b900;">#3</a>: Cross-Sensor Placement 3D Object Detection</strong> of the <a href="https://robosense2025.github.io/" target="_blank"><strong>2025 RoboSense Challenge</strong></a>!
            </p>
            <p>
              Autonomous driving models typically rely on well-calibrated sensor setups, and even slight deviations in
              sensor placements across different vehicles or platforms can significantly degrade performance.
              This lack of robustness to sensor variability makes it challenging to transfer perception models between
              different vehicle platforms without extensive retraining or manual adjustment.
            </p>
            <p>
              Therefore, achieving generalization across sensor placements is essential for the practical deployment of
              the driving-based 3D object detection models.
            </p>
            <p>
              üèÜ <b>Prize Pool:</b> <a style="color: #76b900;"><b>$2,000 USD</b></a> (1st: $1,000, 2nd: $600, 3rd: $400) + Innovation Awards
            </p>
          </div>

          <hr>

          <h2 class="title is-3" style="margin-top: 50px;">üéØ Objective</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              This track evaluates the capability of generalizing perception models across sensor placements. Participants
              are expected to develop models that, when trained on fixed sensor placements, can generalize to diverse
              configurations with minimal performance degradation.
            </p>
          </div>

          <hr>

          <h2 class="title is-3" style="margin-top: 50px;">üóÇÔ∏è Phases & Requirements</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <h4 class="title is-4">Phase <a style="color: #76b900;">#1</a>: Development & Validation</h4>
            <p>
              <strong>Duration:</strong> June <a style="color: #76b900;"><b>15th</b></a>, 2025 (Anywhere on Earth) - August <a style="color: #76b900;"><b>15th</b></a>, 2025 (Anywhere on Earth)
            </p>
            <p>
              In this phase, participants are provided with the <strong>training</strong> and <strong>validation</strong>
              datasets which include diverse LiDAR placements. 
              The ground truth annotations are made available to support
              method development and performance validation.
            </p>
            <p>
              Participants are expected to use these datasets to design, train, and refine their models.
            </p>

            <hr>

            <h4 class="title is-4" style="margin-top: 40px;">Phase <a style="color: #76b900;">#2</a>: Testing & Submission</h4>
            <p>
              <strong>Duration:</strong> August <a style="color: #76b900;"><b>15th</b></a>, 2025 (Anywhere on Earth) - September <a style="color: #76b900;"><b>15th</b></a>, 2025 (Anywhere on Earth)
            </p>
            <p>
              In the second phase, participants will have access to a <strong>test</strong> dataset with LiDAR placements
              different from those in the training and validation sets. Ground truth annotations are <a style="color: #76b900;"><b>not</b></a> provided.
            </p>
            <p>
              The participants are expected to run their developed models on the test set and submit the results in the json format.
              Model performance will be evaluated based on the submitted outputs.
            </p>
            <p>
              The CodaBench <a style="color: #76b900;"><b>evaluation server</b></a> is open at: <a href="https://www.codabench.org/competitions/9284/" target="_blank">https://www.codabench.org/competitions/9284</a>
            </p>
          </div>

          <hr>

          <h2 class="title is-3" style="margin-top: 50px;">üöó Dataset</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              This track uses datasets collected in the CARLA simulator and formatted according to the <strong>nuScenes</strong> data structure.
              Please use the provided <a href="https://github.com/robosense2025/track3">development kit</a> to process the datasets.
            </p>
            <p>
              This toolkit is based on the <code>nuscenes-devkit</code> but has been customized for this track.
              It can be used in the same way as the official nuScenes dataset.
              <strong>Do not</strong> install the official <code>nuscenes-devkit</code>, as it might not be fully compatible with the datasets used in this track.
            </p>
          </div>

          <hr>

          <h2 class="title is-3" style="margin-top: 50px;">üõ†Ô∏è Baseline Model</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              In this track, BEVFusion-L is designated as the baseline method. Participants may choose to build upon the baseline or
              develop their own approaches independently. Detailed environment setup and experimental protocols can be found in the
              <a href="https://github.com/robosense2025/track3" target="_blank" rel="noopener noreferrer"><strong>Track3 GitHub repository</strong></a>.
            </p>
          </div>

          <hr>

          <h2 class="title is-3" style="margin-top: 50px;">üìä Baseline Results</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              In this track, we use ...
            </p>
            <div class="table-container">
              <table class="table is-bordered is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Method</th>
                    <th>mAP</th>
                    <th>mATE</th>
                    <th>mASE</th>
                    <th>mAOE</th>
                    <th>mAVE</th>
                    <th>mAAE</th>
                    <th>NDS</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>BEVFusion-L</td>
                    <td>0.6051</td>
                    <td>0.1210</td>
                    <td>0.1235</td>
                    <td>1.1638</td>
                    <td>2.2521</td>
                    <td>0.3983</td>
                    <td>0.5383</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <hr>

          <h2 class="title is-3" style="margin-top: 50px;">üîó Resources</h2>
          <div class="content has-text-justified" style="padding-top: 0px">
            <p>
              We provide the following resources to support the development of models in this track:
            </p>
            <div class="table-container">
              <table class="table is-bordered is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Resource</th>
                    <th>Link</th>
                    <th>Description</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><strong>GitHub Repository</strong></td>
                    <td><a href="https://github.com/robosense2025/track3" target="_blank">github.com/robosense2025/track3</a></td>
                    <td>Baseline code and setup instructions</td>
                  </tr>
                  <tr>
                    <td><strong>HuggingFace Dataset</strong></td>
                    <td><a href="https://huggingface.co/datasets/robosense/datasets/tree/main/track3-sensor-placement" target="_blank">HuggingFace Dataset</a></td>
                    <td>Dataset with training and test splits</td>
                  </tr>
                  <tr>
                    <td><strong>Checkpoint</strong></td>
                    <td><a href="https://huggingface.co/datasets/robosense/datasets/blob/main/track3-sensor-placement/baseline.pth" target="_blank">Pre-Trained Model</a></td>
                    <td>Weights of the baseline model</td>
                  </tr>
                  <tr>
                    <td><strong>Registration</strong></td>
                    <td><a href="https://docs.google.com/forms/d/e/1FAIpQLSdwfvk-NHdQh9-REiBLCjHMcyLT-sPCOCzJU-ux5jbcZLTkBg/viewform?usp=sharing&ouid=111184461807593368933" target="_blank">Google Form</a>&nbsp;(Closed on August 15th)</td>
                    <td>Team registration for the challenge</td>
                  </tr>
                  <tr>
                    <td><strong>Submission Portal</strong></td>
                    <td><a href="https://www.codabench.org/competitions/9284/" target="_blank">CodaBench Platform</a></td>
                    <td>Online evaluation platform</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>





          <hr>

          <h2 class="title is-3" style="margin-top: 50px;">‚ùì Frequently Asked Questions</h2>
          <div class="content has-text-justified" style="padding-top: 0px">

          <p>
            Here, we provide a list of Frequently Asked Questions (FAQs) below for better clarity. 
            If you have additional questions on the details about this competition, please reach out at <a>robosense2025@gmail.com</a>.
          </p>

          <hr>
            
          <div style="margin-bottom: 25px;">
            <h4 class="title is-5" style="color: #76b900; margin-bottom: 10px;">
              Can we use multi-modal (e.g., camera‚ÄìLiDAR fusion) methods?
            </h4>
            <p style="background-color: #f9f9f9; padding: 15px; border-radius: 8px; border-left: 4px solid #76b900;">
              No. <a style="color: #76b900;"><b>Only</b></a> LiDAR data may be used for model training and evaluation.
              Camera data is provided solely for visualization and must <a style="color: #76b900;"><b>not</b></a> be used for either training or evaluation.
            </p>
          </div>

          <div style="margin-bottom: 25px;">
            <h4 class="title is-5" style="color: #76b900; margin-bottom: 10px;">
              Can we use external data for pre-training, or employ pre-trained models?
            </h4>
            <p style="background-color: #f9f9f9; padding: 15px; border-radius: 8px; border-left: 4px solid #76b900;">
              No. You may only use the data provided by the competition for model training.
            </p>
          </div>

          <div style="margin-bottom: 25px;">
            <h4 class="title is-5" style="color: #76b900; margin-bottom: 10px;">
              How are competition scores ranked?
            </h4>
            <p style="background-color: #f9f9f9; padding: 15px; border-radius: 8px; border-left: 4px solid #76b900;">
              Rankings are determined primarily by mAP. 
              If the difference in mAP between submissions is within 0.01 (1%), then NDS will be used as a secondary criterion to break ties.
            </p>
          </div>





          <hr>

          <h2 class="title is-3" style="margin-top: 50px;">üìñ References</h2>
          <div class="content has-text-justified" style="padding-top: 0px;">
            <div style="
              max-width: 100%;
              overflow-x: auto;
              background-color: #f9f9f9;
              border-radius: 5px;
              padding: 1em;
              font-size: 0.9em;
              box-sizing: border-box;
            ">
              <pre style="
                margin: 0;
                font-family: monospace;
                white-space: pre;
                overflow-x: auto;
              ">
@misc{robosense2025track3,
  title        = {RoboSense Challenge 2025: Track 3 - Sensor Placement},
  author       = {RoboSense Challenge 2025 Organizers},
  year         = {2025},
  howpublished = {https://robosense2025.github.io/track3}
}
              </pre>
            </div>
          </div>




          <hr>





<!-- <h2 class="title is-3">üìñ References</h2>
<div class="content has-text-justified" style="padding-top: 0px;">
  <div style="
    max-width: 100%;
    overflow-x: auto;
    background-color: #f9f9f9;
    border-radius: 5px;
    padding: 1em;
    font-size: 0.9em;
    box-sizing: border-box;
  ">
    <pre style="
      margin: 0;
      font-family: monospace;
      white-space: pre;
      overflow-x: auto;
    ">
@article{xie2025drivebench,
  title     = {},
  author    = {},
  booktitle = {},
  year      = {2025},
  url       = {https://robosense2025.github.io}
}
    </pre>
  </div>
</div> -->




        </div>
      </div>
    </div><br>
  </section>



  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International</a> license.
            </p>
            <p>
              Copyright ¬© <a style="color: #ffc000;">Robo</a><a style="color: #76b900;">Sense</a> 2025 All Rights Reserved.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>


</body>

</html>
